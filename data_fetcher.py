import yfinance as yf
import investpy
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import pytz
import sys
import time
import re
import numpy as np # numpyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import os
import google.generativeai as genai
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

class DataFetcher:
    def __init__(self):
        self.tickers = {
            "S&P500": "^GSPC",
            "NASDAQ100": "^NDX",
            "ãƒ€ã‚¦30": "^DJI",
            "SOX": "^SOX",
            "ç±³å›½2å¹´é‡‘åˆ©": "^TNX", 
            "ç±³å›½10å¹´é‡‘åˆ©": "^TNX",
            "ãƒ‰ãƒ«å††": "JPY=X",
            "ãƒ¦ãƒ¼ãƒ­ãƒ‰ãƒ«": "EURUSD=X",
            "ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³": "BTC-USD",
            "ã‚´ãƒ¼ãƒ«ãƒ‰": "GC=F",
            "åŸæ²¹": "CL=F",
            "VIX": "^VIX"
        }
        self.sector_etfs = {
            "XLK": "Technology Select Sector SPDR Fund",
            "XLF": "Financial Select Sector SPDR Fund",
            "XLV": "Health Care Select Sector SPDR Fund",
            "XLI": "Industrial Select Sector SPDR Fund",
            "XLY": "Consumer Discretionary Select Sector SPDR Fund",
            "XLP": "Consumer Staples Select Sector PDR Fund",
            "XLE": "Energy Select Sector SPDR Fund",
            "XLU": "Utilities Select Sector SPDR Fund",
            "XLB": "Materials Select Sector SPDR Fund",
            "XLRE": "Real Estate Select Sector SPDR Fund",
            "XLC": "Communication Services Select Sector SPDR Fund"
        }
        
        # ç±³çµŒæ¸ˆæŒ‡æ¨™åã®è‹±â†’æ—¥æœ¬èªå¤‰æ›ãƒãƒƒãƒ—
        self.indicator_translations = {
            "Initial Jobless Claims": "æ–°è¦å¤±æ¥­ä¿é™ºç”³è«‹ä»¶æ•°",
            "GDP (QoQ)": "å®Ÿè³ªGDPï¼ˆå‰æœŸæ¯”å¹´ç‡ï¼‰",
            "GDP Price Index (QoQ)": "GDPä¾¡æ ¼æŒ‡æ•°ï¼ˆå‰æœŸæ¯”ï¼‰",
            "Core PCE Price Index (MoM)": "ã‚³ã‚¢PCEä¾¡æ ¼æŒ‡æ•°ï¼ˆå‰æœˆæ¯”ï¼‰",
            "Core PCE Price Index (YoY)": "ã‚³ã‚¢PCEä¾¡æ ¼æŒ‡æ•°ï¼ˆå‰å¹´æ¯”ï¼‰",
            "PCE Price Index (MoM)": "PCEä¾¡æ ¼æŒ‡æ•°ï¼ˆå‰æœˆæ¯”ï¼‰",
            "PCE Price Index (YoY)": "PCEä¾¡æ ¼æŒ‡æ•°ï¼ˆå‰å¹´æ¯”ï¼‰",
            "Existing Home Sales": "ä¸­å¤ä½å®…è²©å£²ä»¶æ•°",
            "New Home Sales": "æ–°ç¯‰ä½å®…è²©å£²ä»¶æ•°",
            "Consumer Confidence": "æ¶ˆè²»è€…ä¿¡é ¼æ„ŸæŒ‡æ•°",
            "ISM Manufacturing PMI": "ISMè£½é€ æ¥­æ™¯æ³æŒ‡æ•°",
            "ISM Non-Manufacturing PMI": "ISMéè£½é€ æ¥­æ™¯æ³æŒ‡æ•°",
            "Retail Sales (MoM)": "å°å£²å£²ä¸Šé«˜ï¼ˆå‰æœˆæ¯”ï¼‰",
            "CPI (MoM)": "æ¶ˆè²»è€…ç‰©ä¾¡æŒ‡æ•°ï¼ˆå‰æœˆæ¯”ï¼‰",
            "CPI (YoY)": "æ¶ˆè²»è€…ç‰©ä¾¡æŒ‡æ•°ï¼ˆå‰å¹´æ¯”ï¼‰",
            "Core CPI (MoM)": "ã‚³ã‚¢æ¶ˆè²»è€…ç‰©ä¾¡æŒ‡æ•°ï¼ˆå‰æœˆæ¯”ï¼‰",
            "Core CPI (YoY)": "ã‚³ã‚¢æ¶ˆè²»è€…ç‰©ä¾¡æŒ‡æ•°ï¼ˆå‰å¹´æ¯”ï¼‰",
            "Unemployment Rate": "å¤±æ¥­ç‡"
        }

        self.ASSET_CLASSES = {
            "US_STOCK": ["^GSPC", "^DJI", "^NDX", "^SOX", "^TNX", "^VIX"],
            "24H_ASSET": ["JPY=X", "EURUSD=X", "BTC-USD", "GC=F", "CL=F"]
        }
        self.INTRADAY_INTERVAL = "5m"
        self.INTRADAY_PERIOD_DAYS = 7

        # æœªè¨³ã®çµŒæ¸ˆæŒ‡æ¨™ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
        self.untranslated_file = os.path.join(os.path.dirname(__file__), "untranslated_indicators.txt")

        # Selenium WebDriverã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
        self.chrome_options = Options()
        self.chrome_options.add_argument("--headless")
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--disable-gpu")
        self.chrome_options.add_argument("--window-size=1920x1080")
        self.chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36')
        
        # ChromeDriverã®ãƒ‘ã‚¹ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã«è¿½åŠ  (Homebrewã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå ´åˆ)
        # sys.path.insert(0,'/usr/local/bin/chromedriver') # Homebrewã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹
        # ã¾ãŸã¯ webdriver_manager ã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç®¡ç†
        # self.driver_service = ChromeService(ChromeDriverManager().install())
        # ä¸Šè¨˜ã¯Colabå‘ã‘ãªã®ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ç›´æ¥ãƒ‘ã‚¹ã‚’æŒ‡å®šã™ã‚‹ã‹ã€
        # webdriver_managerãŒè‡ªå‹•ã§ãƒ‘ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’æœŸå¾…ã™ã‚‹
        # ã¾ãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸchromedriverã®ãƒ‘ã‚¹ã‚’ç’°å¢ƒå¤‰æ•°PATHã«è¿½åŠ ã—ã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹

        # --- Gemini initialisation for news classification ---
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("Warning: GEMINI_API_KEY not set. News country classification will return 'OTHER'.")
            self.gemini_model = None
        else:
            try:
                genai.configure(api_key=api_key)
                chosen = None
                for m in genai.list_models():
                    if 'generateContent' in m.supported_generation_methods:
                        if m.name == 'models/gemini-2.5-flash-lite-preview-06-17':
                            chosen = m.name; break
                        if m.name == 'models/gemini-2.5-flash-preview-05-20':
                            chosen = m.name; break
                        if 'flash' in m.name:
                            chosen = m.name
                if chosen is None:
                    chosen = genai.list_models()[0].name
                self.gemini_model = genai.GenerativeModel(chosen)
                print(f"Gemini model for classification initialised: {chosen}")
            except Exception as e:
                print(f"Warning: Unable to initialise Gemini model: {e}")
                self.gemini_model = None

    def get_market_data(self):
        """ä¸»è¦æŒ‡æ¨™ã®ç›´è¿‘å€¤ã€å‰æ—¥æ¯”ã€å¤‰åŒ–ç‡ã‚’å–å¾—ã™ã‚‹"""
        market_data = {}
        ny_tz = pytz.timezone('America/New_York')
        today_ny = datetime.now(ny_tz)
        
        if today_ny.weekday() == 5: # Saturday
            today_ny = today_ny - timedelta(days=1)
        elif today_ny.weekday() == 6: # Sunday
            today_ny = today_ny - timedelta(days=2)
            
        today = today_ny.date()
        yesterday = today - timedelta(days=1)
        while yesterday.weekday() >= 5: # Saturday or Sunday
            yesterday -= timedelta(days=1)

        for name, ticker in self.tickers.items():
            print(f"--- Market Data: Fetching {name} ({ticker}) ---")
            try:
                if name == "ç±³å›½2å¹´é‡‘åˆ©":
                    data = investpy.get_bond_historical_data(bond='U.S. 2Y', from_date=yesterday.strftime('%d/%m/%Y'), to_date=today.strftime('%d/%m/%Y'))
                    if not data.empty and len(data) > 1:
                        current_value = data['Close'].iloc[-1]
                        previous_value = data['Close'].iloc[-2]
                        change = current_value - previous_value
                        change_bp = change * 100
                        change_percent = (change / previous_value) * 100 if previous_value != 0 else 0
                        market_data[name] = {
                            "current": f"{current_value:.3f}",
                            "change": f"{change:.3f}",
                            "change_bp": f"{change_bp:.2f}",
                            "change_percent": f"{change_percent:.2f}%"
                        }
                        print(f"  âœ… {name} data fetched: Current={current_value:.3f}")
                    else:
                        market_data[name] = {"current": "N/A", "change": "N/A", "change_bp": "N/A", "change_percent": "N/A"}
                        print(f"  âŒ {name} data empty or insufficient.")
                    continue
                
                data = yf.download(ticker, start=yesterday - timedelta(days=5), end=today + timedelta(days=1), progress=False) # auto_adjust=TrueãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                
                print(f"  Raw data columns for {ticker}: {data.columns}")
                # auto_adjust=Trueã®å ´åˆã€MultiIndexã¯é€šå¸¸è¿”ã•ã‚Œãªã„ãŒã€å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯ã¯æ®‹ã™
                if isinstance(data.columns, pd.MultiIndex):
                    print(f"  MultiIndex detected for {ticker}. Flattening columns.")
                    data.columns = data.columns.get_level_values(0)
                    print(f"  Flattened columns for {ticker}: {data.columns}")

                if not data.empty and len(data) >= 2:
                    recent_data = data.tail(2)
                    current_value = recent_data['Close'].iloc[-1]
                    previous_value = recent_data['Close'].iloc[-2]
                    change = current_value - previous_value
                    change_percent = (change / previous_value) * 100 if previous_value != 0 else 0
                    
                    market_data[name] = {
                        "current": f"{current_value:.2f}",
                        "change": f"{change:.2f}",
                        "change_percent": f"{change_percent:.2f}%"
                    }
                    print(f"  âœ… {name} data fetched: Current={current_value:.2f}")
                else:
                    market_data[name] = {"current": "N/A", "change": "N/A", "change_percent": "N/A"}
                    print(f"  âŒ {name} data empty or insufficient.")
            except Exception as e:
                print(f"  âŒ Error fetching data for {name} ({ticker}): {e}")
                market_data[name] = {"current": "N/A", "change": "N/A", "change_percent": "N/A"}
        return market_data

    def get_economic_indicators(self):
        """çµŒæ¸ˆæŒ‡æ¨™ï¼ˆéå»24æ™‚é–“ã«ç™ºè¡¨ã•ã‚ŒãŸã‚‚ã®ã¨ã€ä»Šå¾Œ24æ™‚é–“ã«å…¬è¡¨äºˆå®šã®ã‚‚ã®ï¼‰ã‚’å–å¾—ã™ã‚‹"""
        # economic_data = {"yesterday": [], "today_scheduled": []} # æ—¢å­˜ã®å®šç¾©ã‚’å‰Šé™¤
        
        TARGET_CALENDAR_COUNTRIES = ['united states'] # å®šç¾©ã‚’è¿½åŠ 

        def fetch_and_process_calendar_final():
            # 1. å®Ÿè¡Œæ™‚åˆ»ã‚’åŸºæº–ã«Â±24h ã®æœŸé–“ã‚’è¨ˆç®—
            jst = pytz.timezone('Asia/Tokyo')
            now_jst = datetime.now(jst)
            base_time_jst = now_jst  # å®Ÿè¡Œæ™‚åˆ»ã‚’åŸºæº–

            past_limit_jst = base_time_jst - timedelta(hours=24)
            future_limit_jst = base_time_jst + timedelta(hours=24)

            # ä¼‘æ—¥(é€±æœ«)ã®å ´åˆã¯æ¬¡ã®å–¶æ¥­æ—¥ã¾ã§ future_limit ã‚’å»¶é•·
            while future_limit_jst.weekday() >= 5:  # 5=Sat, 6=Sun
                future_limit_jst += timedelta(days=1)

            from_date = past_limit_jst.strftime('%d/%m/%Y')
            to_date = future_limit_jst.strftime('%d/%m/%Y')

            try:
                df_raw = investpy.economic_calendar(
                    from_date=from_date, to_date=to_date, countries=TARGET_CALENDAR_COUNTRIES
                )
            except Exception as e:
                print(f"  âŒ Error: investpyã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return pd.DataFrame()

            if df_raw.empty:
                print("  Warning: å¯¾è±¡æœŸé–“ã®çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return pd.DataFrame()

            # 2. investpy ã®å…¬è¡¨æ™‚åˆ»ã¯ã™ã¹ã¦ UTC åŸºæº–ã¨ä»®å®šã—ã€UTCâ†’JST ã¸å¤‰æ›ã™ã‚‹
            df_processed = df_raw.copy()

            # 'time' ãŒ 'All Day' ã‚„ç©ºæ¬„ã§ãªã„è¡Œã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹
            df_processed = df_processed[df_processed['time'].str.contains(':', na=False)].copy()

            # --- investpy ã® time ã¯æ—¢ã«æ±äº¬æ™‚é–“ã¨ã¿ãªã™ ---
            # æ—¥ä»˜ã¨æ™‚åˆ»ã®æ–‡å­—åˆ—ã‚’çµåˆã—ã€UTCã®datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            df_processed['datetime_utc'] = pd.to_datetime(
                df_processed['date'] + ' ' + df_processed['time'],
                format='%d/%m/%Y %H:%M',
                errors='coerce'
            ).dt.tz_localize('Asia/Tokyo')

            df_processed.dropna(subset=['datetime_utc'], inplace=True) # å¤‰æ›å¤±æ•—è¡Œã‚’å‰Šé™¤

            # 3. åŸºæº–æ™‚åˆ»ã‚’ä¸­å¿ƒã« 24h å‰å¾Œ (+ä¼‘æ—¥è£œæ­£) ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            df_filtered = df_processed[
                (df_processed['datetime_utc'] >= past_limit_jst) &
                (df_processed['datetime_utc'] <= future_limit_jst)
            ].copy()

            if df_filtered.empty:
                print("  Warning: éå»24æ™‚é–“ï½æœªæ¥24æ™‚é–“ã®ç¯„å›²ã«è©²å½“ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return pd.DataFrame()

            # 4. ã€Œç™ºè¡¨æ¸ˆã¿ã€ã€Œç™ºè¡¨äºˆå®šã€ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¿½åŠ 
            df_filtered['çŠ¶æ…‹'] = np.where(df_filtered['datetime_utc'] < base_time_jst, 'ç™ºè¡¨æ¸ˆã¿', 'ç™ºè¡¨äºˆå®š')

            # 5. è¡¨ç¤ºç”¨ã«JSTã®æ—¥æ™‚åˆ—ã‚’ä½œæˆ
            jst = pytz.timezone('Asia/Tokyo')
            df_filtered['æ—¥æ™‚(JST)'] = df_filtered['datetime_utc'].dt.strftime('%Y-%m-%d %H:%M')

            # 6. æœ€çµ‚çš„ãªã‚«ãƒ©ãƒ ã‚’é¸æŠãƒ»æ•´å½¢
            column_rename_map = {'zone': 'å›½', 'event': 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'importance': 'é‡è¦åº¦', 'actual': 'ç™ºè¡¨å€¤', 'forecast': 'äºˆæƒ³å€¤', 'previous': 'å‰å›å€¤'}
            df_filtered.rename(columns=column_rename_map, inplace=True)
            final_cols = ['çŠ¶æ…‹', 'æ—¥æ™‚(JST)', 'å›½', 'é‡è¦åº¦', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ç™ºè¡¨å€¤', 'äºˆæƒ³å€¤', 'å‰å›å€¤']

            df_final = df_filtered[[col for col in final_cols if col in df_filtered.columns]]
            # --- æŒ‡æ¨™åç¿»è¨³ã¨æœªè¨³ãƒ­ã‚° ---
            df_final['ã‚¤ãƒ™ãƒ³ãƒˆ_EN'] = df_final['ã‚¤ãƒ™ãƒ³ãƒˆ']
            df_final['ã‚¤ãƒ™ãƒ³ãƒˆ'] = df_final['ã‚¤ãƒ™ãƒ³ãƒˆ_EN'].apply(lambda x: self.indicator_translations.get(x, x))
            untranslated = set(df_final[df_final['ã‚¤ãƒ™ãƒ³ãƒˆ'] == df_final['ã‚¤ãƒ™ãƒ³ãƒˆ_EN']]['ã‚¤ãƒ™ãƒ³ãƒˆ_EN'].unique())
            self._log_untranslated_indicators(untranslated)

            return df_final.sort_values(by='æ—¥æ™‚(JST)')

        # --- é–¢æ•°ã®å®Ÿè¡Œ ---
        df_economic_calendar = fetch_and_process_calendar_final()

        economic_data = {"yesterday": [], "today_scheduled": []} # ã“ã“ã§åˆæœŸåŒ–

        if not df_economic_calendar.empty:
            # 'ç™ºè¡¨æ¸ˆã¿' ã®ãƒ‡ãƒ¼ã‚¿ã‚’ 'yesterday' ã«
            for _, row in df_economic_calendar[df_economic_calendar['çŠ¶æ…‹'] == 'ç™ºè¡¨æ¸ˆã¿'].iterrows():
                economic_data["yesterday"].append({
                    "name": row['ã‚¤ãƒ™ãƒ³ãƒˆ'],
                    "time": row['æ—¥æ™‚(JST)'],
                    "previous": row.get('å‰å›å€¤', 'N/A'),
                    "actual": row.get('ç™ºè¡¨å€¤', 'N/A'),
                    "forecast": row.get('äºˆæƒ³å€¤', 'N/A')
                })
            # 'ç™ºè¡¨äºˆå®š' ã®ãƒ‡ãƒ¼ã‚¿ã‚’ 'today_scheduled' ã«
            for _, row in df_economic_calendar[df_economic_calendar['çŠ¶æ…‹'] == 'ç™ºè¡¨äºˆå®š'].iterrows():
                economic_data["today_scheduled"].append({
                    "name": row['ã‚¤ãƒ™ãƒ³ãƒˆ'],
                    "time": row['æ—¥æ™‚(JST)'],
                    "previous": row.get('å‰å›å€¤', 'N/A'),
                    "forecast": row.get('äºˆæƒ³å€¤', 'N/A')
                })
            print(f"  âœ… Economic indicators fetched: {len(economic_data['yesterday'])} announced, {len(economic_data['today_scheduled'])} scheduled.")
        else:
            print("  âŒ Economic calendar data could not be generated.")
            
        # æœªè¨³ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚‚ã“ã“ã§è¨˜éŒ²ï¼ˆå¿µã®ãŸã‚ï¼‰
        self._log_untranslated_indicators({row['name'] for cat in economic_data.values() for row in cat if row['name'] not in self.indicator_translations.values()})
        return economic_data

    def _log_untranslated_indicators(self, indicators: set):
        """æœªè¨³ã®çµŒæ¸ˆæŒ‡æ¨™åã‚’é‡è¤‡ãªããƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜"""
        if not indicators:
            return
        try:
            existing = set()
            if os.path.exists(self.untranslated_file):
                with open(self.untranslated_file, 'r', encoding='utf-8') as f:
                    existing = {line.strip() for line in f if line.strip()}
            new_items = indicators - existing
            if not new_items:
                return
            with open(self.untranslated_file, 'a', encoding='utf-8') as f:
                for item in sorted(new_items):
                    f.write(item + "\n")
            print(f"  ğŸ”– Logged {len(new_items)} untranslated indicator(s).")
        except Exception as e:
            print(f"Warning: Unable to log untranslated indicators: {e}")

    def get_sector_etf_performance(self):
        """ç±³å›½ã®ã‚»ã‚¯ã‚¿ãƒ¼ETFã®å¤‰åŒ–ç‡ã‚’å–å¾—ã™ã‚‹"""
        sector_performance = {}
        ny_tz = pytz.timezone('America/New_York')
        today_ny = datetime.now(ny_tz)
        
        if today_ny.weekday() == 5: # Saturday
            today_ny = today_ny - timedelta(days=1)
        elif today_ny.weekday() == 6: # Sunday
            today_ny = today_ny - timedelta(days=2)
            
        today = today_ny.date()
        yesterday = today - timedelta(days=1)
        while yesterday.weekday() >= 5: # Saturday or Sunday
            yesterday -= timedelta(days=1)

        for ticker, name in self.sector_etfs.items():
            print(f"--- Sector ETF: Fetching {name} ({ticker}) ---")
            try:
                data = yf.download(ticker, start=yesterday - timedelta(days=5), end=today + timedelta(days=1), progress=False, auto_adjust=False)
                
                print(f"  Raw data columns for {ticker}: {data.columns}")
                if isinstance(data.columns, pd.MultiIndex):
                    print(f"  MultiIndex detected for {ticker}. Flattening columns.")
                    data.columns = data.columns.get_level_values(0)
                    print(f"  Flattened columns for {ticker}: {data.columns}")

                if not data.empty and len(data) >= 2:
                    recent_data = data.tail(2)
                    current_value = recent_data['Close'].iloc[-1]
                    previous_value = recent_data['Close'].iloc[-2]
                    change_percent = ((current_value - previous_value) / previous_value) * 100 if previous_value != 0 else 0
                    sector_performance[name] = round(change_percent, 2) if change_percent is not None else None
                    print(f"  âœ… {name} data fetched: {change_percent:.2f}%")
                else:
                    sector_performance[name] = "N/A"
                    print(f"  âŒ {name} data empty or insufficient.")
            except Exception as e:
                print(f"  âŒ Error fetching data for sector ETF {name} ({ticker}): {e}")
                sector_performance[name] = "N/A"
        return sector_performance

    def scrape_reuters_news(self, query: str, hours_limit: int = 24,
                            max_pages: int = 5, items_per_page: int = 20,
                            target_categories: list = None, exclude_keywords: list = None) -> list:
        """ãƒ­ã‚¤ã‚¿ãƒ¼ã®ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã‚’åˆ©ç”¨ã—ã¦è¨˜äº‹æƒ…å ±ã‚’åé›†ã™ã‚‹ (Seleniumä½¿ç”¨)"""
        articles_data, processed_urls = [], set()
        base_search_url = "https://jp.reuters.com/site-search/"
        if target_categories is None: target_categories = []
        if exclude_keywords is None: exclude_keywords = []
        
        driver = None
        print("\n--- ãƒ­ã‚¤ã‚¿ãƒ¼è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ---")
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.implicitly_wait(15)
            driver.set_page_load_timeout(120)
            jst = pytz.timezone('Asia/Tokyo')
            time_threshold_jst = datetime.now(jst) - timedelta(hours=hours_limit)
            print(f"  [DEBUG] ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åŸºæº–æ™‚åˆ»: {time_threshold_jst.strftime('%Y-%m-%d %H:%M')}")

            for page_num in range(max_pages):
                offset = page_num * items_per_page
                search_url = f"{base_search_url}?query={requests.utils.quote(query)}&offset={offset}"
                print(f"  ãƒ­ã‚¤ã‚¿ãƒ¼: ãƒšãƒ¼ã‚¸ {page_num + 1}/{max_pages} ã‚’å‡¦ç†ä¸­... (URL: {search_url})")
                driver.get(search_url)
                time.sleep(7)
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                articles_on_page = soup.find_all('li', attrs={"data-testid": "StoryCard"})

                print(f"  [DEBUG] ãƒšãƒ¼ã‚¸ {page_num + 1} ã§ {len(articles_on_page)} ä»¶ã®è¨˜äº‹å€™è£œï¼ˆliã‚¿ã‚°ï¼‰ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                if not articles_on_page:
                    if page_num == 0: print("    [!] æœ€åˆã®ãƒšãƒ¼ã‚¸ã§è¨˜äº‹å€™è£œãŒå…¨ãè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒˆã®HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚")
                    break

                for i, article_li in enumerate(articles_on_page):
                    # num_articlesã«ã‚ˆã‚‹åˆ¶é™ã‚’å‰Šé™¤
                    # if len(articles_data) >= num_articles:
                    #     break
                    # print(f"\n  --- å€™è£œ {i+1} ã®è©³ç´°ãƒã‚§ãƒƒã‚¯ ---") # DEBUG
                    title, article_url, article_time_jst, category_text = "å–å¾—å¤±æ•—", "å–å¾—å¤±æ•—", None, "ä¸æ˜"

                    title_container = article_li.find('div', class_=re.compile(r'title__title'))
                    link_element = title_container.find('a', attrs={"data-testid": "TitleLink"}) if title_container else None
                    if link_element and link_element.has_attr('href'):
                        article_url = link_element.get('href', '')
                        if article_url.startswith('/'): article_url = "https://jp.reuters.com" + article_url
                    # print(f"    [DEBUG] URL: {article_url}") # DEBUG

                    if link_element:
                        title = link_element.get_text(strip=True)
                    # print(f"    [DEBUG] Title: {title}") # DEBUG

                    time_element = article_li.find('time', attrs={"data-testid": "DateLineText"})
                    if time_element and time_element.has_attr('datetime'):
                        try:
                            dt_utc = datetime.fromisoformat(time_element.get('datetime').replace('Z', '+00:00'))
                            article_time_jst = dt_utc.astimezone(jst)
                            # print(f"    [DEBUG] Time: {article_time_jst.strftime('%Y-%m-%d %H:%M')}") # DEBUG
                        except (ValueError, AttributeError):
                            pass
                    # else:
                        # print("    [DEBUG] Time: timeã‚¿ã‚°ã¾ãŸã¯datetimeå±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“") # DEBUG

                    kicker = article_li.find('span', attrs={"data-testid": "KickerLabel"})
                    # ãƒ­ã‚¤ã‚¿ãƒ¼ã®ã‚«ãƒ†ã‚´ãƒªåãŒå¤‰å‹•ã™ã‚‹ãŸã‚ã€ã‚ˆã‚ŠæŸ”è»Ÿã«ãƒã‚§ãƒƒã‚¯
                    category_text_raw = kicker.get_text(strip=True) if kicker else "ä¸æ˜"
                    category_text = category_text_raw.replace(" category", "").replace("Category", "").strip()
                    # print(f"    [DEBUG] Category: {category_text}") # DEBUG

                    # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ®µéšã®ãƒ‡ãƒãƒƒã‚° ---
                    if not article_url.startswith('http') or article_url in processed_urls:
                        print(f"    [ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼] ä¸æ­£ãªURL ({article_url}) ã‹ã€æ—¢ã«å‡¦ç†æ¸ˆã¿ã®URLã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue
                    if article_time_jst is None:
                        print(f"    [ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼] æ—¥æ™‚ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue
                    if article_time_jst < time_threshold_jst:
                        print(f"    [ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼] è¨˜äº‹ãŒå¤ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ (è¨˜äº‹æ™‚åˆ»: {article_time_jst.strftime('%Y-%m-%d %H:%M')})")
                        continue
                    if any(keyword.lower() in title.lower() for keyword in exclude_keywords):
                        print(f"    [ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼] é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ({exclude_keywords}) ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ (ã‚¿ã‚¤ãƒˆãƒ«: {title})")
                        continue
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡å®šã—ãŸtarget_categoriesã®å½¢å¼ã‚’å°Šé‡ã—ã¤ã¤ã€æŸ”è»Ÿã«ãƒã‚§ãƒƒã‚¯
                    if target_categories and not any(tc.lower().replace("category", "").strip() in category_text.lower() for tc in target_categories):
                        print(f"    [ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼] ã‚«ãƒ†ã‚´ãƒª '{category_text}' ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒª ({target_categories}) ã«å«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue

                    # --- æˆåŠŸ ---
                    print("    >>> [æˆåŠŸ] å…¨ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ã¾ã—ãŸã€‚è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚")
                    body_text = self._scrape_reuters_article_body(article_url) or ""
                    country_code = self.classify_country(f"{title}\n{body_text}")
                    articles_data.append({
                        'title': title, 'url': article_url, 'published_jst': article_time_jst,
                        'category': category_text, 'country': country_code,
                        'body': body_text if body_text else "[æœ¬æ–‡å–å¾—å¤±æ•—/ç©º]"
                    })
                    processed_urls.add(article_url)

                if len(articles_on_page) < items_per_page: break
                time.sleep(1)
        except Exception as e:
            print(f"  ãƒ­ã‚¤ã‚¿ãƒ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # import traceback; traceback.print_exc() # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ãŒå¿…è¦ãªå ´åˆ
        finally:
            if driver: driver.quit()
        print(f"--- ãƒ­ã‚¤ã‚¿ãƒ¼è¨˜äº‹å–å¾—å®Œäº†: {len(articles_data)} ä»¶ ---")
        return articles_data

    def classify_country(self, text: str) -> str:
        """Gemini API ã‚’ç”¨ã„ã¦è¨˜äº‹ã®é–¢é€£å›½ã‚’åˆ¤å®šã— 2ã€œ3 æ–‡å­—ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™ã€‚å¤±æ•—æ™‚ã¯ 'OTHER'"""
        if not self.gemini_model:
            return "OTHER"
        prompt = (
            "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯çµŒæ¸ˆãƒ»ãƒãƒ¼ã‚±ãƒƒãƒˆé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã§ã™ã€‚"\
            "ä¸»ã«é–¢ä¿‚ã™ã‚‹å›½ã‚’è‹±èª 2 æ–‡å­—(US, JP, CN, EU, UK ãªã©) ã§ 1 ã¤ã ã‘å›ç­”ã—ã¦ãã ã•ã„ã€‚"\
            "ã‚‚ã—ç‰¹å®šãŒé›£ã—ã‘ã‚Œã° OTHER ã¨ç­”ãˆã¦ãã ã•ã„ã€‚"\
            "å›ç­”ã¯å›½ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ 1 è¡Œã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n---\n" + text[:1800] + "\n---\n")
        try:
            resp = self.gemini_model.generate_content(prompt)
            code = resp.text.strip().upper()
            import re
            m = re.match(r"[A-Z]{2,3}", code)
            return m.group(0) if m else "OTHER"
        except Exception as e:
            print(f"Gemini classify_country error: {e}")
            return "OTHER"

    def _scrape_reuters_article_body(self, article_url: str) -> str:
        """æŒ‡å®šã•ã‚ŒãŸãƒ­ã‚¤ã‚¿ãƒ¼è¨˜äº‹URLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹ (requestsä½¿ç”¨)"""
        print(f"    [DEBUG] Attempting to scrape article body from: {article_url}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'}
            response = requests.get(article_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, 'html.parser')
            
            body_container = soup.find('div', class_='article-body__content__17Yit')
            if not body_container:
                print(f"    [DEBUG] Body container not found for {article_url}. Trying alternative selectors.")
                body_container = soup.find('div', class_='article-body')
                if not body_container:
                    body_container = soup.find('div', class_='text__text__1FZnP')
            
            if not body_container:
                print(f"    [DEBUG] No suitable body container found for {article_url}.")
                return ""

            paragraphs = [p_div.get_text(separator=' ', strip=True) for p_div in body_container.find_all('div', attrs={"data-testid": lambda x: x and x.startswith('paragraph-')})]
            if not paragraphs:
                print(f"    [DEBUG] No paragraphs found with data-testid for {article_url}. Trying generic p tags.")
                paragraphs = [p.get_text(separator=' ', strip=True) for p in body_container.find_all('p')]

            article_text = '\n'.join(paragraphs)
            cleaned_text = re.sub(r'\s+', ' ', article_text).strip()
            
            if not cleaned_text:
                print(f"    [DEBUG] Cleaned article text is empty for {article_url}.")
            else:
                print(f"    [DEBUG] Successfully scraped {len(cleaned_text)} characters from {article_url}.")
            
            return cleaned_text
        except requests.exceptions.RequestException as e:
            print(f"    [DEBUG] Request error scraping article body from {article_url}: {e}")
            return ""
        except Exception as e:
            print(f"    [DEBUG] General error scraping article body from {url}: {e}")
            return ""

    def get_historical_data(self, ticker, period="1y", interval="1d"):
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®éå»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã€‚
        period: '1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max'
        interval: '1m', '2m', '5m', '15m', '30m', '60m', '90m', '1h', '1d', '5d', '1wk', '1mo', '3mo'
        """
        print(f"--- Historical Data: Fetching {ticker} ({period}, {interval}) ---")
        try:
            data = yf.download(ticker, period=period, interval=interval, progress=False, auto_adjust=False)
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            
            if data.empty:
                print(f"  Warning: Empty raw historical data for {ticker}.")
                return pd.DataFrame()

            # MultiIndexã‚’å¹³å¦åŒ–
            if isinstance(data.columns, pd.MultiIndex):
                print(f"  MultiIndex detected for {ticker}. Flattening columns.")
                data.columns = data.columns.get_level_values(0)
                print(f"  Flattened columns for {ticker}: {data.columns}")

            if not all(col in data.columns for col in required_cols):
                print(f"  Warning: Missing required columns in historical data for {ticker}. Columns: {data.columns.tolist()}")
                return pd.DataFrame()

            for col in ['Open', 'High', 'Low', 'Close']:
                data[col] = pd.to_numeric(data[col], errors='coerce')
            data['Volume'] = pd.to_numeric(data['Volume'], errors='coerce').fillna(0).astype(int)
            data.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)
            
            if data.empty:
                print(f"  Warning: Historical data for {ticker} became empty after cleaning.")
                return pd.DataFrame()
            print(f"  âœ… Historical data for {ticker} fetched: {len(data)} rows.")
            # print(f"  Historical data head for {ticker}:\n{data.head()}")
            # print(f"  Historical data info for {ticker}:\n{data.info()}")
            return data
        except Exception as e:
            print(f"  âŒ Error fetching historical data for {ticker}: {e}")
            # import traceback
            # traceback.print_exc()
            return pd.DataFrame()

    def get_intraday_data(self, ticker):
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ã‚¤ãƒ³ãƒˆãƒ©ãƒ‡ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã€‚
        æ¨ªè»¸ã¯æ±äº¬æ™‚é–“ã§è¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«å‡¦ç†ã™ã‚‹ã€‚
        """
        jst = pytz.timezone('Asia/Tokyo')
        ny_tz = pytz.timezone('America/New_York')
        utc = pytz.utc

        print(f"--- Intraday Data: Fetching {ticker} ({self.INTRADAY_INTERVAL} for {self.INTRADAY_PERIOD_DAYS} days) ---")
        try:
            df_raw = yf.download(
                ticker, period=f"{self.INTRADAY_PERIOD_DAYS}d", interval=self.INTRADAY_INTERVAL,
                progress=False, auto_adjust=False
            )
            if df_raw.empty:
                print(f"  Warning: Empty raw intraday data for {ticker}.")
                return pd.DataFrame()

            df_cleaned = df_raw.copy()
            print(f"  Raw intraday data columns for {ticker}: {df_cleaned.columns}")

            # MultiIndexã‚’å¹³å¦åŒ–ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
            if isinstance(df_cleaned.columns, pd.MultiIndex):
                print(f"  MultiIndex detected for {ticker}. Flattening columns.")
                df_cleaned.columns = df_cleaned.columns.get_level_values(0)
                print(f"  Flattened columns for {ticker}: {df_cleaned.columns}")

            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
            ohlcv_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            if not all(col in df_cleaned.columns for col in ohlcv_cols):
                print(f"  Warning: Missing required OHLCV columns in intraday data for {ticker}. Columns: {df_cleaned.columns.tolist()}")
                return pd.DataFrame()

            for col in ohlcv_cols:
                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')
            df_cleaned.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)

            if df_cleaned.empty:
                print(f"  Warning: Intraday data for {ticker} became empty after cleaning.")
                return pd.DataFrame()

            df_processed = df_cleaned.reset_index()
            datetime_col = 'Datetime' if 'Datetime' in df_processed.columns else 'index'
            df_processed[datetime_col] = pd.to_datetime(df_processed[datetime_col])

            if df_processed[datetime_col].dt.tz is None:
                df_processed[datetime_col] = df_processed[datetime_col].dt.tz_localize(utc)
            else:
                df_processed[datetime_col] = df_processed[datetime_col].dt.tz_convert(utc)

            df_final = pd.DataFrame()

            if ticker in self.ASSET_CLASSES['US_STOCK']:
                print(f"  Info: {ticker} is US_STOCK. Processing for latest NY trading day.")
                df_processed['æ—¥æ™‚_NY'] = df_processed[datetime_col].dt.tz_convert(ny_tz)
                df_processed['å–å¼•æ—¥_NY'] = df_processed['æ—¥æ™‚_NY'].dt.normalize()
                latest_trading_day_ny = df_processed['å–å¼•æ—¥_NY'].max()
                print(f"  Info: Latest trading day (NY time) for {ticker} is {latest_trading_day_ny.strftime('%Y-%m-%d')}.")
                df_final = df_processed[df_processed['å–å¼•æ—¥_NY'] == latest_trading_day_ny].copy()

            elif ticker in self.ASSET_CLASSES['24H_ASSET']:
                print(f"  Info: {ticker} is 24H_ASSET. Processing for JST 7am start.")
                df_processed['æ—¥æ™‚_JST'] = df_processed[datetime_col].dt.tz_convert(jst)
                now_jst = datetime.now(jst)
                today_7am_jst = now_jst.replace(hour=7, minute=0, second=0, microsecond=0)
                start_time_jst = today_7am_jst - timedelta(days=1) if now_jst < today_7am_jst else today_7am_jst
                end_time_jst = start_time_jst + timedelta(days=1)
                print(f"  Info: Extraction period (JST) for {ticker}: {start_time_jst.strftime('%Y-%m-%d %H:%M')} to {end_time_jst.strftime('%Y-%m-%d %H:%M')}.")
                df_final = df_processed[(df_processed['æ—¥æ™‚_JST'] >= start_time_jst) & (df_processed['æ—¥æ™‚_JST'] < end_time_jst)].copy()
            else:
                print(f"  Info: {ticker} is neither US_STOCK nor 24H_ASSET. Converting to JST directly.")
                df_final = df_processed.copy()
                df_final['æ—¥æ™‚'] = df_final[datetime_col].dt.tz_convert(jst)


            if df_final.empty:
                print(f"  Warning: {ticker} ã®å¯¾è±¡æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return pd.DataFrame()

            if 'æ—¥æ™‚' not in df_final.columns:
                 df_final['æ—¥æ™‚'] = df_final[datetime_col].dt.tz_convert(jst)

            final_cols = ['æ—¥æ™‚', 'Open', 'High', 'Low', 'Close', 'Volume']
            final_cols_existing = [col for col in final_cols if col in df_final.columns]
            df_final = df_final[final_cols_existing]

            intraday_chart_data = df_final.set_index('æ—¥æ™‚')
            print(f"  âœ… Intraday data for {ticker} fetched: {len(intraday_chart_data)} rows.")
            # print(f"  Intraday data head for {ticker}:\n{intraday_chart_data.head()}")
            # print(f"  Intraday data info for {ticker}:\n{intraday_chart_data.info()}")
            return intraday_chart_data

        except Exception as e:
            print(f"  âŒ Error fetching intraday data for {ticker}: {e}")
            # import traceback
            # traceback.print_exc()
            return pd.DataFrame()

if __name__ == "__main__":
    fetcher = DataFetcher()
    
    print("--- Market Data ---")
    market_data = fetcher.get_market_data()
    for name, data in market_data.items():
        print(f"{name}: Current={data['current']}, Change={data.get('change', 'N/A')}, Change_BP={data.get('change_bp', 'N/A')}, Change_Percent={data.get('change_percent', 'N/A')}")

    print("\n--- Economic Indicators ---")
    economic_indicators = fetcher.get_economic_indicators()
    print("Yesterday's Announcements:")
    for item in economic_indicators["yesterday"]:
        print(f"  {item['name']}: Previous={item['previous']}, Actual={item['actual']}, Forecast={item['forecast']}")
    print("Today's Scheduled:")
    for item in economic_indicators["today_scheduled"]:
        print(f"  {item['name']}: Previous={item['previous']}, Actual={item['actual']}, Forecast={item['forecast']}")

    print("\n--- Sector ETF Performance ---")
    sector_performance = fetcher.get_sector_etf_performance()
    for name, perf in sector_performance.items():
        print(f"{name}: {perf}")

    print("\n--- Reuters News ---")
    news = fetcher.scrape_reuters_news(query="ç±³å›½å¸‚å ´", hours_limit=72, max_pages=5, items_per_page=20, target_categories=["ãƒ“ã‚¸ãƒã‚¹", "ãƒãƒ¼ã‚±ãƒƒãƒˆ", "ãƒˆãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹", "ãƒ¯ãƒ¼ãƒ«ãƒ‰", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ã‚¢ã‚¸ã‚¢å¸‚å ´", "çµŒæ¸ˆ"], exclude_keywords=["ã‚¹ãƒãƒ¼ãƒ„", "ã‚¨ãƒ³ã‚¿ãƒ¡", "äº”è¼ª", "ã‚µãƒƒã‚«ãƒ¼", "æ˜ ç”»", "å°†æ£‹", "å›²ç¢", "èŠ¸èƒ½", "ãƒ©ã‚¤ãƒ•", "ã‚¢ãƒ³ã‚°ãƒ«ï¼š"])
    for article in news:
        print(f"Title: {article['title']}\nLink: {article['link']}\n")

    print("\n--- Historical Data (S&P500 1 year) ---")
    sp500_hist = fetcher.get_historical_data("^GSPC", period="1y")
    print(sp500_hist.head())

    print("\n--- Intraday Data (S&P500 1 day) ---")
    sp500_intraday = fetcher.get_intraday_data("^GSPC", days=1)
    print(sp500_intraday.head())
